train:
  batch_size: 1024
  context_len: 4096
  num_epochs: 5
  lr: 0.0004
  train_file: "data/lyrics.txt"
  cv_ratio: 0.1
  num_workers: 6
  device: "cuda"
  log_interval: 10
  num_train_samples: 100
  num_val_samples: 10
  step_size: 20         # Reduce LR every 10 epochs.
  scheduler_gamma: 0.8  # Multiply LR by this factor every step.
model:
  vocab_size: 37
  in_dim: 1024
  num_heads: 8
  n_layers: 8
  dropout: 0.2
  compile_model: true

generate:
  num_tokens: 500
  input_text: " "  
  weights_dir: "checkpoints"